{
    "name": "Convert to Qualcomm NPU",
    "evalMetrics": {
        "encoder-latency-avg": "encoder-latency-avg",
        "decoder-latency-avg": "decoder-latency-avg"
    },
    "memoryGbSuggested": 30,
    "executeRuntimeFeatures": [
        "QAI"
    ],
    "evaluationRuntimeFeatures": [
        "QAI"
    ],
    "addCpu": true,
    "runtime": {
        "autoGenerated": true,
        "name": "Evaluate on",
        "type": "enum",
        "displayNames": [
            "Qualcomm NPU",
            "CPU"
        ],
        "path": "systems.target_system.accelerators.0.execution_providers.0",
        "values": [
            "QNNExecutionProvider",
            "CPUExecutionProvider"
        ],
        "actions": [
            [
                {
                    "type": "update",
                    "path": "systems.target_system.accelerators.0.execution_providers.0",
                    "value": "QNNExecutionProvider"
                },
                {
                    "type": "update",
                    "path": "systems.target_system.accelerators.0.device",
                    "value": "npu"
                }
            ],
            [
                {
                    "type": "update",
                    "path": "systems.target_system.accelerators.0.execution_providers.0",
                    "value": "CPUExecutionProvider"
                },
                {
                    "type": "update",
                    "path": "systems.target_system.accelerators.0.device",
                    "value": "cpu"
                }
            ]
        ],
        "readOnly": false
    },
    "optimizationPaths": [
        {
            "path": "passes.aitkpython.precision",
            "name": "WeightType"
        },
        {
            "path": "passes.aitkpython.activation_type",
            "name": "ActivationType"
        }
    ],
    "optimizationDefault": "w8a16",
    "aitkPython": "qnn_workflow.py",
    "sections": [
        {
            "autoGenerated": true,
            "name": "Convert",
            "phase": "Conversion",
            "parameters": [],
            "toggle": {
                "autoGenerated": true,
                "name": "Convert to ONNX format",
                "type": "bool",
                "path": "passes.aitkpython",
                "actions": [
                    [],
                    []
                ],
                "readOnly": true
            }
        },
        {
            "name": "Quantize",
            "phase": "Quantization",
            "parameters": [
                {
                    "name": "Activation Type",
                    "tags": [
                        "ActivationType"
                    ],
                    "description": "Quantization data type of activation. ‘Int8’ for signed 8-bit integer, ‘UInt8’ for unsigned 8-bit integer etc.",
                    "descriptionLink": "https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html",
                    "type": "enum",
                    "displayNames": [
                        "Int8",
                        "UInt8",
                        "Int16",
                        "UInt16"
                    ],
                    "displayType": "RadioGroup",
                    "path": "passes.aitkpython.activation_type",
                    "values": [
                        "int8",
                        "uint8",
                        "int16",
                        "uint16"
                    ],
                    "template": {
                        "path": "passes.aitkpython.activation_type",
                        "template": "ActivationType"
                    }
                },
                {
                    "name": "Weight Type",
                    "tags": [
                        "WeightType"
                    ],
                    "description": "Data type for quantizing weights. ‘Int8’ for signed 8-bit integer, ‘UInt8’ for unsigned 8-bit integer etc.",
                    "descriptionLink": "https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html",
                    "type": "enum",
                    "displayNames": [
                        "Int8",
                        "UInt8",
                        "Int16",
                        "UInt16"
                    ],
                    "displayType": "RadioGroup",
                    "path": "passes.aitkpython.precision",
                    "values": [
                        "int8",
                        "uint8",
                        "int16",
                        "uint16"
                    ],
                    "template": {
                        "path": "passes.aitkpython.precision",
                        "template": "WeightType"
                    }
                },
                {
                    "name": "Quantization Dataset",
                    "tags": [
                        "QuantizationDataset"
                    ],
                    "type": "enum",
                    "path": "passes.aitkpython.dataset_name",
                    "values": [
                        "librispeech_asr"
                    ],
                    "template": {
                        "path": "passes.aitkpython.dataset_name",
                        "values": [
                            "librispeech_asr"
                        ],
                        "template": "QuantizationDataset"
                    }
                },
                {
                    "name": "Quantization Dataset Split",
                    "tags": [
                        "QuantizationDatasetSplit",
                        "DependsOnDataset"
                    ],
                    "type": "enum",
                    "path": "passes.aitkpython.split",
                    "values": [
                        "test",
                        "validation",
                        "train.100",
                        "train.360"
                    ],
                    "template": {
                        "path": "passes.aitkpython.split",
                        "values": [
                            "test",
                            "validation",
                            "train.100",
                            "train.360"
                        ],
                        "template": "QuantizationDatasetSplit"
                    }
                },
                {
                    "name": "Quantization Dataset Size",
                    "description": "ATTENTION! Using 100 samples of librispeech dataset to generate the required real data requires around 164 GB of disk space.",
                    "type": "int",
                    "path": "passes.aitkpython.length",
                    "template": {
                        "description": "ATTENTION! Using 100 samples of librispeech dataset to generate the required real data requires around 164 GB of disk space.",
                        "path": "passes.aitkpython.length",
                        "template": "QuantizationDatasetSize"
                    }
                }
            ],
            "disableToggleGeneration": true,
            "toggle": {
                "name": "Quantize model",
                "type": "bool",
                "path": "passes.aitkpython",
                "actions": [
                    [],
                    []
                ],
                "readOnly": true
            }
        },
        {
            "name": "Evaluate",
            "phase": "Evaluation",
            "parameters": [],
            "toggle": {
                "autoGenerated": true,
                "name": "Evaluate model performance",
                "type": "bool",
                "path": "evaluator",
                "actions": [
                    [],
                    [
                        {
                            "type": "delete",
                            "path": "evaluator"
                        }
                    ]
                ]
            }
        }
    ]
}
