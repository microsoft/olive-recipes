{
    "input_model": {
        "type": "HfModel",
        "model_path": "openai/clip-vit-large-patch14",
        "task": "zero-shot-image-classification",
        "load_kwargs": {
            "attn_implementation": "eager"
        },
        "io_config": {
            "input_names": [
                "input_ids",
                "pixel_values",
                "attention_mask"
            ],
            "input_shapes": [
                [
                    10,
                    77
                ],
                [
                    1,
                    3,
                    224,
                    224
                ],
                [
                    10,
                    77
                ]
            ],
            "input_types": [
                "int64",
                "float32",
                "int64"
            ],
            "output_names": [
                "logits_per_image",
                "logits_per_text",
                "text_embeds",
                "image_embeds"
            ],
            "output_shapes": [
                [
                    1,
                    10
                ],
                [
                    10,
                    1
                ],
                [
                    10,
                    512
                ],
                [
                    1,
                    512
                ]
            ]
        }
    },
    "systems": {
        "target_system": {
            "type": "LocalSystem",
            "accelerators": [
                {
                    "device": "npu",
                    "execution_providers": [
                        "QNNExecutionProvider"
                    ]
                }
            ]
        }
    },
    "data_configs": [
        {
            "name": "quant_data_config",
            "user_script": "user_script.py",
            "load_dataset_config": {
                "type": "clip_dataset",
                "model_name": "openai/clip-vit-large-patch14",
                "dataset_name": "nlphuji/flickr30k",
                "start": 0,
                "length": 100
            },
            "dataloader_config": {
                "type": "no_auto_batch_dataloader"
            }
        },
        {
            "name": "metric_data_config",
            "user_script": "user_script.py",
            "load_dataset_config": {
                "type": "clip_dataset",
                "model_name": "openai/clip-vit-large-patch14",
                "dataset_name": "nlphuji/flickr30k",
                "start": 100,
                "length": 100
            },
            "dataloader_config": {
                "type": "no_auto_batch_dataloader"
            },
            "post_process_data_config": {
                "type": "clip_post_process"
            }
        }
    ],
    "evaluators": {
        "common_evaluator": {
            "metrics": [
                {
                    "name": "accuracy",
                    "type": "accuracy",
                    "backend": "huggingface_metrics",
                    "data_config": "metric_data_config",
                    "sub_types": [
                        {
                            "name": "accuracy",
                            "priority": 1
                        }
                    ]
                },
                {
                    "name": "latency",
                    "type": "latency",
                    "data_config": "metric_data_config",
                    "sub_types": [
                        {
                            "name": "avg"
                        },
                        {
                            "name": "max"
                        },
                        {
                            "name": "min"
                        }
                    ]
                }
            ]
        }
    },
    "passes": {
        "conversion": {
            "type": "OnnxConversion",
            "target_opset": 20,
            "save_as_external_data": true
        },
        "surgery": {
            "type": "GraphSurgeries",
            "surgeries": [
                {
                    "surgeon": "MatMulAddToGemm"
                },
                {
                    "surgeon": "ReplaceAttentionMaskValue",
                    "replacement": -50.0
                }
            ]
        },
        "quantization": {
            "type": "OnnxStaticQuantization",
            "quant_preprocess": true,
            "data_config": "quant_data_config",
            "activation_type": "uint16",
            "precision": "uint8",
            "calibrate_method": "MinMax",
            "save_as_external_data": true
        }
    },
    "evaluator": "common_evaluator",
    "evaluate_input_model": false,
    "target": "target_system",
    "clean_cache": false,
    "output_dir": "model/clip_qnn",
    "cache_dir": "cache"
}
