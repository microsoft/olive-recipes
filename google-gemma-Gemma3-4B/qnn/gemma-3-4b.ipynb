{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 3 4B QNN model conversion with Olive \n",
    "### Task: Text + Vision Generation üìù\n",
    "\n",
    "In this notebook, you'll:\n",
    "- Download the required datasets\n",
    "- Convert LLM to QNN format\n",
    "- Convert Vision to QNN format\n",
    "- Convert Embedding layer with image to QNN format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform requirements\n",
    "This notebook is intended to run on a machine with:\n",
    "  * **Operating System**: Linux Ubuntu 22.04 (automated setup script is Linux-only)\n",
    "  * **Python**: 3.10\n",
    "  * NVIDIA driver version equivalent to 525.60.13\n",
    "  * NVIDIA A100 GPU\n",
    "  * **Storage**: ~13GB for COCO train2017 dataset (downloaded automatically)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üêç Python Virtual environments\n",
    "Creates Olive and QNN python virtual environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/CodeLinaro/Olive.git -b dev/qti-kromero/gemma3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import venv\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import json\n",
    "import shutil\n",
    "import urllib.request\n",
    "import onnx\n",
    "from onnx import helper, TensorProto\n",
    "import glob\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "MODEL=\"google/gemma-3-4b-it\"\n",
    "OLIVE_PYTHON_PATH = './olive_venv'\n",
    "OLIVE_PYTHON_BIN = './olive_venv/bin/python'\n",
    "olive_pip_path = Path(OLIVE_PYTHON_PATH) / \"bin\" / \"pip\"\n",
    "OLIVE_REPO_PATH = Path(\"./Olive\")\n",
    "OLIVE_REQ = \"./requirements.txt\"\n",
    "QNN_REQ = \"./qnn_req.txt\"\n",
    "\n",
    "QNN_PYTHON_PATH = './qnn_venv'\n",
    "QNN_PYTHON_BIN_PATH = './qnn_venv/bin'\n",
    "qnn_pip_path = Path(QNN_PYTHON_PATH) / \"bin\" / \"pip\"\n",
    "QNN_PYTHON_BIN_FULL_PATH = f\"{current_dir}/{QNN_PYTHON_BIN_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Olive Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(OLIVE_PYTHON_PATH):\n",
    "    print(\"Creating Olive Venv\")\n",
    "    builder = venv.EnvBuilder(with_pip=True)\n",
    "    builder.create(Path(OLIVE_PYTHON_PATH))\n",
    "my_env = os.environ.copy()\n",
    "my_env[\"BUILD_CUDA_EXT\"] = \"0\"\n",
    "GPTQ=\"git+https://github.com/ModelCloud/GPTQModel.git@558449bed3ef2653c36041650d30da6bbbca440d\"\n",
    "subprocess.check_call([str(olive_pip_path), \"install\", \"-U\", \"-r\" , OLIVE_REQ], env=my_env)\n",
    "subprocess.check_call([str(olive_pip_path), \"install\", \"--no-build-isolation\", GPTQ], env=my_env)\n",
    "subprocess.check_call([str(olive_pip_path), \"install\", \"-e\", OLIVE_REPO_PATH])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare QNN Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(QNN_PYTHON_PATH):\n",
    "    print(\"Creating QNN Venv\")\n",
    "    builder = venv.EnvBuilder(with_pip=True)\n",
    "    builder.create(Path(QNN_PYTHON_PATH))\n",
    "subprocess.check_call([str(qnn_pip_path), \"install\", \"--no-build-isolation\", \"-r\" , QNN_REQ], env=my_env)\n",
    "subprocess.check_call([str(qnn_pip_path), \"install\", \"-e\", OLIVE_REPO_PATH])\n",
    "subprocess.check_call([str(qnn_pip_path), \"install\", \"-U\", \"--pre\", \"--extra-index-url\",\n",
    "                       \"https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple\",\n",
    "                       \"onnxruntime-qnn==1.23.0.dev20250815002\", \"--no-deps\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ó Login to Hugging Face\n",
    "To access models, you'll need to log-in to Hugging Face with a [user access token](https://huggingface.co/docs/hub/security-tokens). The following command will run you through the steps to login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token <>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply few patches to Onnxruntime\n",
    "\n",
    "This is needed for running the Olive recipies for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://raw.githubusercontent.com/CodeLinaro/onnxruntime/326d9d30129bbad698e0306d24dcea0ec5a19e60\"\n",
    "urls = [\n",
    "    base_url + \"/onnxruntime/python/tools/quantization/execution_providers/qnn/quant_config.py\",\n",
    "    base_url + \"/onnxruntime/python/tools/quantization/quant_utils.py\"\n",
    "]\n",
    "\n",
    "destinations = [\n",
    "    OLIVE_PYTHON_PATH+\"/lib/python3.10/site-packages/onnxruntime/quantization/execution_providers/qnn/quant_config.py\",\n",
    "    OLIVE_PYTHON_PATH+\"/lib/python3.10/site-packages/onnxruntime/quantization/quant_utils.py\"\n",
    "]\n",
    "\n",
    "for url, dest in zip(urls, destinations):\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "    print(f\"Downloaded and replaced: {dest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Olive Recipes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU utilization observed during the run**\n",
    "\n",
    "\t\ta. Text GPTQModel quantization:        12gb\n",
    "\t\tb. Text Onnx static quantization:      41gb\n",
    "\t\tc. Vision Onnx static quantization:    68gb\n",
    "        d. Embedding Onnx static quantization: 3gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Context binary directories if they exist\n",
    "def clean_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        for file in glob.glob(os.path.join(path, '*')):\n",
    "            if os.path.isfile(file):\n",
    "                os.remove(file)\n",
    "dirs_to_clean = [\n",
    "    './models/gemma3_qnn/model/',\n",
    "    './models/gemma-3-4b-it-vision/model/',\n",
    "    './models/gemma-3-4b-it-embed/model/'\n",
    "]\n",
    "\n",
    "for dir_path in dirs_to_clean:\n",
    "    clean_directory(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ LLM model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(f\"./gemma3-4b-text-qnn-config.json\")\n",
    "with open(config_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "data[\"systems\"][\"qnn_system\"][\"python_environment_path\"] = QNN_PYTHON_BIN_FULL_PATH\n",
    "data[\"input_model\"][\"model_path\"] = MODEL\n",
    "\n",
    "with open(config_path, \"w\") as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./olive_venv/bin/olive run --config ./gemma3-4b-text-qnn-config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Vision model Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(f\"./gemma3-4b-vision-qnn-config.json\")\n",
    "with open(config_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "data[\"systems\"][\"qnn_system\"][\"python_environment_path\"] = QNN_PYTHON_BIN_FULL_PATH\n",
    "\n",
    "with open(config_path, \"w\") as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./olive_venv/bin/olive run --config ./gemma3-4b-vision-qnn-config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./olive_venv/bin/olive run --config ./gemma3-4b-embedding-qnn-config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep output of the embedding model as uint16 instead of float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load(\"./models/gemma-3-4b-it-embed/model/model.onnx\")\n",
    "graph = model.graph\n",
    "\n",
    "last_node = graph.node[-1]\n",
    "graph.node.remove(last_node)\n",
    "previous_node_output = graph.node[-1].output[0]\n",
    "new_output = helper.make_tensor_value_info(\n",
    "    name=previous_node_output,\n",
    "    elem_type=TensorProto.UINT16,\n",
    "    shape=[\"batch_size\", \"seq_length\", 2560]\n",
    ")\n",
    "graph.output.remove(graph.output[0])\n",
    "graph.output.extend([new_output])\n",
    "onnx.save(model, \"./models/gemma-3-4b-it-embed/model/embeddings_with_image.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare final ORT GenAI folder for on-device inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./models/gemma-3-4b-it-embed/model/embeddings_with_image.onnx ./models/gemma3_qnn/model/\n",
    "!cp ./models/gemma-3-4b-it-vision/model/model_ctx.onnx ./models/gemma3_qnn/model/model_ctx_vision.onnx \n",
    "!cp ./models/gemma-3-4b-it-vision/model/model_ctx_qnn.bin ./models/gemma3_qnn/model/model_ctx_qnn.bin \n",
    "!cp ./genai/*.* ./models/gemma3_qnn/model/\n",
    "!ls -al ./models/gemma3_qnn/model/\n",
    "\n",
    "print(\"ORT GenAI inference setup: ./models/gemma3_qnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
