{
    "input_model": {
        "type": "HfModel",
        "model_path": "google/gemma-3-4b-it",
        "custom_task_class_name": "Gemma3ForCausalLM",
        "custom_task_class_module": "transformers"
    },
    "systems": {
        "qnn_system": {
            "type": "PythonEnvironment",
            "python_environment_path": "",
            "accelerators": [ { "execution_providers": [ "QNNExecutionProvider" ] } ]
        }
    },
    "data_configs": [
        {
            "name": "gemma_data_config",
            "user_script": "custom_gemma3_4b_datasets.py",
            "load_dataset_config": { "type": "gemma_dataset", "model_id": "google/gemma-3-4b-it" }
        }
    ],
    "passes": {
        "g": {
            "type": "GptqModel",
            "bits": 4,
            "sym": true,
            "group_size": -1,
            "lm_head": false,
            "device": "cuda",
            "data_config": "gemma_data_config"
        },
        "cs": { "type": "CaptureSplitInfo", "num_splits": 2, "unique_embeds_lm_head_splits": true },
        "mb": {
            "type": "ModelBuilder",
            "precision": "int4",
            "int4_block_size": 32,
            "int4_accuracy_level": 4,
            "int4_op_types_to_quantize": [ "MatMul", "Gather" ]
        },
        "mq": {
            "type": "MatMulNBitsToQDQ",
            "use_int4": true,
            "add_zero_point": true,
            "nodes_to_exclude": [ "/lm_head/MatMul_Q4" ],
            "save_as_external_data": true
        },
        "gs": {
            "type": "GraphSurgeries",
            "surgeries": [
                { "surgeon": "RemoveRopeMultiCache" },
                { "surgeon": "AttentionMaskToSequenceLengths" },
                { "surgeon": "SimplifiedLayerNormToL2Norm" }
            ],
            "save_as_external_data": true
        },
        "sq": {
            "type": "OnnxStaticQuantization",
            "data_config": "gemma_data_config",
            "activation_type": "uint16",
            "precision": "uint8",
            "calibration_providers": [ "CUDAExecutionProvider" ],
            "quant_preprocess": true,
            "op_types_to_exclude": [ "GatherBlockQuantized", "GroupQueryAttention", "MatMulNBits" ],
            "save_as_external_data": true
        },
        "sp": { "type": "SplitModel" },
        "st": { "type": "StaticLLM", "batch_size": 1, "context_length": 64 },
        "cb": {
            "type": "EPContextBinaryGenerator",
            "provider_options": {
                "htp_performance_mode": "burst",
                "htp_graph_finalization_optimization_mode": "3",
                "vtcm_mb": "8",
                "htp_arch": "v73",
                "soc_model": "60"
            },
            "session_options": { "intra_op_num_threads": 2, "inter_op_num_threads": 1 },
            "weight_sharing": true
        },
        "cp": { "type": "ComposeOnnxModels" }
    },
    "target": "qnn_system",
    "log_severity_level": 0,
    "output_dir": "models/gemma3_qnn",
    "cache_dir": "cache",
    "no_artifacts": true
}
