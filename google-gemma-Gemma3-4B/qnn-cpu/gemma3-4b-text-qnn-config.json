{
    "input_model": {
        "type": "HfModel",
        "model_path": "google/gemma-3-4b-it",
        "custom_task_class_name": "Gemma3ForCausalLM",
        "custom_task_class_module": "transformers",
        "untie_lm_head_weights": true
    },
    "systems": {
        "qnn_system": {
            "type": "PythonEnvironment",
            "python_environment_path": "",
            "accelerators": [
                {
                    "execution_providers": [
                        "QNNExecutionProvider"
                    ]
                }
            ]
        }
    },
    "data_configs": [
        {
            "name": "wikitext2_train_joined",
            "type": "HuggingfaceContainer",
            "load_dataset_config": {
                "data_name": "wikitext",
                "subset": "wikitext-2-raw-v1",
                "split": "train"
            },
            "pre_process_data_config": {
                "strategy": "join",
                "add_special_tokens": false,
                "max_seq_len": 4096,
                "max_samples": 128
            }
        },
        {
            "name": "wikitext2_train_act",
            "type": "HuggingfaceContainer",
            "load_dataset_config": {
                "data_name": "wikitext",
                "subset": "wikitext-2-raw-v1",
                "split": "train"
            },
            "pre_process_data_config": {
                "strategy": "line-by-line",
                "add_special_tokens": true,
                "max_samples": 256,
                "max_seq_len": 4096
            }
        }
    ],
    "passes": {
        "cs": {
            "type": "CaptureSplitInfo",
            "num_splits": 2,
            "unique_embeds_lm_head_splits": true
        },
        "g": {
            "type": "GptqModel",
            "bits": 4,
            "sym": true,
            "group_size": 128,
            "lm_head": true,
            "desc_act": false,
            "device": "cuda",
            "data_config": "wikitext2_train_joined",
            "dynamic": {
                "+:.*lm_head*": {
                    "bits": 8,
                    "sym": true,
                    "group_size": 16,
                    "desc_act": false
                },
                "+:.*v_proj*": {
                    "bits": 8,
                    "sym": true,
                    "group_size": 32,
                    "desc_act": false
                },
                "+:.*k_proj*": {
                    "bits": 8,
                    "sym": true,
                    "group_size": 32,
                    "desc_act": false
                },
                "+:.*q_proj*": {
                    "bits": 8,
                    "sym": true,
                    "group_size": 32,
                    "desc_act": false
                }
            }
        },
        "mb": {
            "type": "ModelBuilder",
            "precision": "int4",
            "int4_block_size": 16,
            "int4_accuracy_level": 4,
            "int4_op_types_to_quantize": [
                "Gather"
            ],
            "extra_options": {
                "use_packed_matmul": true
            }
        },
        "gs": {
            "type": "GraphSurgeries",
            "surgeries": [
                {
                    "surgeon": "RemoveRopeMultiCache"
                },
                {
                    "surgeon": "AttentionMaskToSequenceLengths"
                },
                {
                    "surgeon": "SimplifiedLayerNormToL2Norm"
                }
            ],
            "save_as_external_data": true
        },
        "sp": {
            "type": "SplitModel",
            "save_as_external_data": false
        },
        "st": {
            "type": "StaticLLM",
            "batch_size": 1,
            "context_length": 64,
            "save_as_external_data": true
        },
        "cp": {
            "type": "ComposeOnnxModels"
        }
    },
    "target": "qnn_system",
    "log_severity_level": 0,
    "output_dir": "models/gemma3_qnn",
    "cache_dir": "cache",
    "no_artifacts": true
}
