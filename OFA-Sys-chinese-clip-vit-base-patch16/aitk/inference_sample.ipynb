{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb33f1a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "onnx_model_path = \"./model/model.onnx\"\n",
    "use_float16 = False\n",
    "\n",
    "ExecutionProvider=\"OpenVINOExecutionProvider\"\n",
    "\n",
    "if ExecutionProvider == \"OpenVINOExecutionProvider\":\n",
    "    onnx_model_path = \"./model/openvino_model_quant_st.onnx\"\n",
    "elif ExecutionProvider == \"DmlExecutionProvider\" or ExecutionProvider == \"NvTensorRTRTXExecutionProvider\" or ExecutionProvider == \"MIGraphXExecutionProvider\":\n",
    "    use_float16 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bb9b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# reference: https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/tutorial?tabs=python#acquiring-the-model-and-preprocessing\n",
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import onnxruntime as ort\n",
    "\n",
    "def register_execution_providers():\n",
    "    worker_script = os.path.abspath('winml.py')\n",
    "    result = subprocess.check_output([sys.executable, worker_script], text=True)\n",
    "    paths = json.loads(result)\n",
    "    for item in paths.items():\n",
    "        ort.register_execution_provider_library(item[0], item[1])\n",
    "\n",
    "register_execution_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fcca8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    " \n",
    "from transformers import ChineseCLIPProcessor\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch\n",
    " \n",
    "processor = ChineseCLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n",
    " \n",
    "url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Squirtle, Bulbasaur, Charmander, Pikachu in English\n",
    "inputs = processor(text=[\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"],\n",
    "                images=image, return_tensors=\"np\", padding=\"max_length\",\n",
    "                max_length= 77, truncation=True)\n",
    " \n",
    "\n",
    "def add_ep_for_device(session_options, ep_name, device_type, ep_options=None):\n",
    "    ep_devices = ort.get_ep_devices()\n",
    "    for ep_device in ep_devices:\n",
    "        if ep_device.ep_name == ep_name and ep_device.device.type == device_type:\n",
    "            print(f\"Adding {ep_name} for {device_type}\")\n",
    "            session_options.add_provider_for_devices([ep_device], {} if ep_options is None else ep_options)\n",
    "            break\n",
    " \n",
    "opts = ort.SessionOptions()\n",
    " \n",
    "add_ep_for_device(opts, ExecutionProvider, ort.OrtHardwareDeviceType.NPU)\n",
    "assert opts.has_providers()\n",
    "\n",
    "# options = ort.SessionOptions()\n",
    "session = ort.InferenceSession(onnx_model_path,\n",
    "    sess_options=opts,\n",
    "    # providers=[ExecutionProvider],\n",
    "    # provider_options=[provider_options]\n",
    ")\n",
    "logits_per_image = session.run([\"logits_per_image\"],\n",
    "                     {\n",
    "                        \"input_ids\": inputs['input_ids'].astype(np.int64),\n",
    "                        \"attention_mask\": inputs['attention_mask'].astype(np.int64),\n",
    "                        \"pixel_values\": inputs['pixel_values'].astype(np.float16) if use_float16 else inputs['pixel_values']\n",
    "                    })\n",
    " \n",
    "probs = torch.tensor(logits_per_image[0]).softmax(dim=1)\n",
    "print(\"Label probs:\", probs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
