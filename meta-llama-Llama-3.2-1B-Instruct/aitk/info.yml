keywords:
    aitk
arch: llama3
recipes:
    - file: "llama3_2_qnn_config.json"
      device: npu
      ep: QNNExecutionProvider
      aitk:
        oliveFile: "o:llama3/qnn/llama3.2_1b_instruct_qnn_config.json"
    - file: "llama3_2_vitis_ai_config.json"
      device: npu
      ep: VitisAIExecutionProvider
      aitk:
        oliveFile: "VitisAI/Llama-3.2-1B-Instruct_quark_vitisai_llm.json"
        requirements: AMD/Quark_py3.10.17
        evalRuntime: AMDNPU
        isGPUSuggested: true
    - file: "llama3_2_ov_config.json"
      devices:
        - npu
      ep: OpenVINOExecutionProvider
    - file: "llama3_2_ov_gpu_config.json"
      devices:
        - gpu
        - cpu
      ep: OpenVINOExecutionProvider
    - file: "llama3_2_trtrtx_config.json"
      device: gpu
      ep: NvTensorRTRTXExecutionProvider
      aitk:
        oliveFile: "NvTensorRtRtx/Llama-3.2-1B-Instruct_model_builder_fp16.json"
    - file: "llama3_2_dml_config.json"
      device: gpu
      ep: DmlExecutionProvider
    - file: "llama3_2_qnn_gpu_config.json"
      device: gpu
      ep: QNNExecutionProvider
      aitk:
        oliveFile: "QNN/config_gpu.json"
        requirementsPatches:
          - AutoGptq
        isGPURequired: true
        runtimeOverwrite:
          executeEp: NvTensorRTRTXExecutionProvider
aitk:
    modelInfo:
        id: "huggingface/meta-llama/Llama-3.2-1B-Instruct"
        version: 6
        p0: true
