olive-ai
onnxruntime-genai-cuda
onnxruntime-gpu
onnxruntime_extensions
# optimum 1.17.0 for fp16 inference
optimum>=1.17.0
transformers>=4.34.99
